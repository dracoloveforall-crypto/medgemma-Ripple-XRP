{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2UTHAMOTkwt"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Fine-tune MedGemma with Reinforcement Learning and TRL\n",
        "\n",
        "\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/rl_with_trl.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"\u003e\u003cbr\u003e Run in Google Colab\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogle-Health%2Fmedgemma%2Fmain%2Fnotebooks%2Frl_with_trl.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"\u003e\u003cbr\u003e Run in Colab Enterprise\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://github.com/google-health/medgemma/blob/main/notebooks/rl_with_trl.ipynb\"\u003e\n",
        "      \u003cimg alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"\u003e\u003cbr\u003e View on GitHub\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\"\u003e\n",
        "      \u003cimg alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"\u003e\u003cbr\u003e View on Hugging Face\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n",
        "\n",
        "This notebook demonstrates RL-tuning MedGemma on a text dataset for medical QA using `trl`: Transformer Reinforcement Learning.\n",
        "\n",
        "In this guide, you will use [`trl`](https://huggingface.co/docs/trl/grpo_trainer) - Transformer Reinforcement Learning to train the model with Reinforcement Learning (RL), specifically, [GRPO (Group Relative Policy Optimization)](https://arxiv.org/abs/2402.03300), utilizing [Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685v2) to reduce computational costs while maintaining high performance.\n",
        "\n",
        "**Citations:**\n",
        "\n",
        "- LoRA: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... \u0026 Chen, W. (2022). Lora: Low-rank adaptation of large language models. ICLR, 1(2), 3.\n",
        "\n",
        "- GRPO: Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., ... \u0026 Guo, D. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you'll need to have a runtime with sufficient resources to fine-tune the MedGemma model. **Note:** This guide requires a GPU that supports bfloat16 data type and has at least 40 GB of memory.\n",
        "\n",
        "You can run this notebook in Google Colab using an A100 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **â–¾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **A100 GPU**.\n",
        "\n",
        "*Note that this will take a long time to run (~10 hrs per epoch of MedQA train split on an A100 40GB).*"
      ],
      "metadata": {
        "id": "T6p-RkIQFQuu"
      }
    },
    {
      "metadata": {
        "id": "kbtSvyCTM6zz"
      },
      "cell_type": "markdown",
      "source": [
        "### Get access to MedGemma\n",
        "\n",
        "Before you get started, make sure that you have access to MedGemma models on Hugging Face:\n",
        "\n",
        "1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).\n",
        "2. Head over to the [MedGemma model page](https://huggingface.co/google/medgemma-4b-it) and accept the usage conditions."
      ]
    },
    {
      "metadata": {
        "id": "FsOaAGBYM9Ac"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Configure your HF token\n",
        "\n",
        "Generate a Hugging Face `write` access token by going to [settings](https://huggingface.co/settings/tokens). **Note:** Make sure that the token has write access to push the fine-tuned model to Hugging Face Hub.\n",
        "\n",
        "If you are using Google Colab, add your access token to the Colab Secrets manager to securely store it. If not, proceed to run the cell below to authenticate with Hugging Face.\n",
        "\n",
        "1. Open your Google Colab notebook and click on the ðŸ”‘ Secrets tab in the left panel. \u003cimg src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%\u003e\n",
        "2. Create a new secret with the name `HF_TOKEN`.\n",
        "3. Copy/paste your token key into the Value input box of `HF_TOKEN`.\n",
        "4. Toggle the button on the left to allow notebook access to the secret.\n",
        "5. Set `HF_HOME` for downstream `trl` runs as well."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\"):\n",
        "    # Use secret if running in Google Colab\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "else:\n",
        "    # Store Hugging Face data under `/content` if running in Colab Enterprise\n",
        "    if os.environ.get(\"VERTEX_PRODUCT\") == \"COLAB_ENTERPRISE\":\n",
        "        os.environ[\"HF_HOME\"] = \"/content/hf\"\n",
        "    # Authenticate with Hugging Face\n",
        "    from huggingface_hub import get_token\n",
        "    if get_token() is None:\n",
        "        from huggingface_hub import notebook_login\n",
        "        notebook_login()"
      ],
      "metadata": {
        "id": "KAyyWj98iFzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfnglnZUTkwu"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhlHYZysncF-"
      },
      "outputs": [],
      "source": [
        "! pip install -U -q transformers trl[vllm] datasets\n",
        "# tested with python 3.12 and !pip install transformers==4.55.3 trl[vllm]==0.21.0 datasets==4.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You may also want to store training runs in your drive\n",
        "# (HIGHLY recommended saving model checkpoints)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dX4MUh4fO0wf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "## Dataset processing\n",
        "\n",
        "This notebook uses the [MedQA](https://arxiv.org/abs/2009.13081) dataset, a multiple-choice question dataset derived from medical licensing exams in the US, China, and Taiwan, designed to assess medical knowledge and clinical reasoning skills.\n",
        "\n",
        "Load the data using the Hugging Face `datasets` library. Then, create train and validation splits. We subsample the dev split for faster evaluation times.\n",
        "\n",
        "**Dataset citation:** Jin, D., Pan, E., Oufattole, N., Weng, W. H., Fang, H., \u0026 Szolovits, P. (2021). What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14), 6421."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "big_prompt = f\"\"\"Answer the given question. Think step by step.\n",
        "You can directly provide the answer (A single letter) inside \u003canswer\u003e and \u003c/answer\u003e (e.g. \u003canswer\u003eA\u003c/answer\u003e), without further additions.\n",
        "Question: [QUESTION]\n",
        "[OPTIONS]\n",
        "\"\"\"\n",
        "\n",
        "def process_medqa(data):\n",
        "  return data.map(lambda x: {\n",
        "                      'prompt': [\n",
        "                          {'role': 'user', 'content': big_prompt.replace('[QUESTION]', x['data']['Question']).replace(\n",
        "                              '[OPTIONS]', f\"(A) {x['data']['Options']['A']} (B) {x['data']['Options']['B']} (C) {x['data']['Options']['C']} (D) {x['data']['Options']['D']}\")}\n",
        "                      ],\n",
        "                      'answer': x['data']['Correct Option']\n",
        "                  })\n",
        "\n",
        "medqa_dataset = datasets.load_dataset(\"openlifescienceai/medqa\")\n",
        "train_dataset = process_medqa(medqa_dataset[\"train\"])\n",
        "val_dataset = process_medqa(medqa_dataset[\"dev\"].shuffle(seed=42).select(range(100)))"
      ],
      "metadata": {
        "id": "gblttaNEwmo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-train the model with LoRA via GRPO on MedQA\n",
        "\n",
        "Traditional fine-tuning of large language models is resource-intensive because it requires adjusting billions of parameters. Parameter-Efficient Fine-Tuning (PEFT) addresses this by training a smaller number of parameters. A common PEFT technique is *Low-Rank Adaptation (LoRA)*, which efficiently adapts large language models by training small, low-rank matrices that are added to the original model instead of updating the full-weight matrices.\n",
        "\n",
        "*GRPO (Group Relative Policy Optimization)* is a reinforcement learning (RL) algorithm that aims to improve efficiency and reduce training costs by eliminating the need for a separate value function. Instead, GRPO uses group-based advantage estimation and incorporates KL divergence into the loss function for better stability.\n",
        "\n",
        "This notebook demonstrates RL training Gemma as well as MedGemma (with verifiable rewards) with LoRA."
      ],
      "metadata": {
        "id": "SwzOobP8OTGK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KihAIYFJjTeR"
      },
      "source": [
        "First, define the reward function to check when the model's answer letter matches the correct answer letter (i.e. 'A', 'B', 'C', or 'D')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [],
      "source": [
        "def extract_xml_answer(text: str) -\u003e str:\n",
        "    answer = text.split(\"\u003canswer\u003e\")[-1]\n",
        "    answer = answer.split(\"\u003c/answer\u003e\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -\u003e list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print(f\"-----Question:\\n{q}\\nAnswer:\\n{answer[0]}\\nResponse:\\n{responses[0]}\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    print([(r,a, r == a) for r, a in zip(extracted_responses, answer)])\n",
        "    return [1.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "Next, configure training with the `GRPOConfig`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import LoraConfig\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "ckpt = \"google/medgemma-4b-it\"\n",
        "# ckpt = \"google/gemma-3-4b-it\"\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"./tuned_medgemma4b\",\n",
        "    eval_on_start=True,                      # Run an evaluation at the very beginning of training.\n",
        "    learning_rate=3e-5,                      # The initial learning rate for the AdamW optimizer.\n",
        "    optim=\"adamw_torch_fused\",               # Optimizer to use; 'adamw_torch_fused' is a faster, memory-efficient AdamW.\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=64,          # Accumulate gradients for this many steps to simulate a larger batch size (per_device_train_batch_size * gradient_accumulation_steps).\n",
        "    num_generations=4,                       # Number of completions to generate per prompt for GRPO's preference learning.\n",
        "    max_prompt_length=512,                   # Maximum token length for input prompts.\n",
        "    max_completion_length=512,               # Maximum token length for the model's generated completions.\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    report_to=\"tensorboard\",\n",
        "    use_vllm=True,                           # Use the vLLM library for significantly faster inference during generation.\n",
        "    vllm_mode=\"colocate\",                    # vLLM deployment mode; 'colocate' runs vLLM on the same GPU(s) as the trainer.\n",
        "    vllm_gpu_memory_utilization=.35,         # Fraction of GPU memory that vLLM is allowed to use (35%).\n",
        "    bf16=True,                               # Enable bfloat16 mixed precision training to save memory and speed up training.\n",
        "    gradient_checkpointing=True,             # Save memory by trading compute (avoids storing all intermediate activations).\n",
        "    gradient_checkpointing_kwargs={\n",
        "        \"use_reentrant\": False               # Use a more efficient implementation of gradient checkpointing.\n",
        "    },\n",
        "    model_init_kwargs={\n",
        "        \"device_map\": \"auto\",\n",
        "        \"dtype\": torch.bfloat16,             # Set model parameter data type to bfloat16.\n",
        "        \"attn_implementation\": \"eager\"       # Gemma 3 recommends using the 'eager' attention implementation.\n",
        "    },\n",
        "    push_to_hub=True\n",
        ")\n",
        "\n",
        "# LoRA to reduce VRAM\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=64,\n",
        "    lora_alpha=64,\n",
        "    target_modules=\"all-linear\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model.\n",
        "\n",
        "Note that this will take a long time to run (~10 hrs per epoch)."
      ],
      "metadata": {
        "id": "jKx8iw7n4DP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=ckpt,\n",
        "    reward_funcs=[correctness_reward_func],\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir=training_args.output_dir)"
      ],
      "metadata": {
        "id": "5Us06pUdoAkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the relevant paths to store training results.\n",
        "! cp -r ./tuned_gemma4b/ /content/drive/MyDrive/trl_colab_storage/"
      ],
      "metadata": {
        "id": "EKyeDdVQGWZ7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize training curves\n",
        "! pip install tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/tuned_medgemma4b/"
      ],
      "metadata": {
        "id": "eRM7_RBrB2PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation: Effect of RL-tuning\n",
        "\n",
        "**Important:** Before you continue, you may need to restart the runtime due to the VRAM limitation on Colab kernels.\n",
        "\n",
        "The following cells compute and print the accuracy of the baseline and fine-tuned models on the test dataset to assess the effect of RL-tuning.\n",
        "\n",
        "We also load and process the test split using the same logic as before, repeated below for convenience."
      ],
      "metadata": {
        "id": "7dobbKZuhe_T"
      }
    },
    {
      "metadata": {
        "id": "yVBrWg5MjOZ_"
      },
      "cell_type": "code",
      "source": [
        "# Reinstantiate env variables upon reload\n",
        "import os\n",
        "import datasets\n",
        "if os.environ.get(\"VERTEX_PRODUCT\") == \"COLAB_ENTERPRISE\":\n",
        "    os.environ[\"HF_HOME\"] = \"/content/hf\"\n",
        "\n",
        "# Load test split\n",
        "big_prompt = f\"\"\"Answer the given question. Think step by step.\n",
        "You can directly provide the answer (A single letter) inside \u003canswer\u003e and \u003c/answer\u003e (e.g. \u003canswer\u003eA\u003c/answer\u003e), without further additions.\n",
        "Question: [QUESTION]\n",
        "[OPTIONS]\n",
        "\"\"\"\n",
        "\n",
        "def process_medqa(data):\n",
        "  return data.map(lambda x: {\n",
        "                      'prompt': [\n",
        "                          {'role': 'user', 'content': big_prompt.replace('[QUESTION]', x['data']['Question']).replace(\n",
        "                              '[OPTIONS]', f\"(A) {x['data']['Options']['A']} (B) {x['data']['Options']['B']} (C) {x['data']['Options']['C']} (D) {x['data']['Options']['D']}\")}\n",
        "                      ],\n",
        "                      'answer': x['data']['Correct Option']\n",
        "                  })\n",
        "  \n",
        "medqa_dataset_test = datasets.load_dataset(\"openlifescienceai/medqa\", split='test')\n",
        "test_dataset = process_medqa(medqa_dataset_test)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def run_inference_batched(test_dataset, model, processor, batch_size=4, device=\"cuda\", verbose=True):\n",
        "    \"\"\"\n",
        "    Runs inference on a processed test dataset using batching for efficiency.\n",
        "\n",
        "    Args:\n",
        "        test_dataset: A dataset where each item has 'prompt' (chat history) and 'answer' (ground truth).\n",
        "        model: The loaded PEFT model for inference.\n",
        "        processor: The processor for tokenizing the input.\n",
        "        batch_size (int): The number of samples to process at once. Adjust based on VRAM.\n",
        "        device (str): The device to run inference on ('cuda' or 'cpu').\n",
        "        verbose (bool): Whether to print progress and sample outputs.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, with each dictionary containing the prompt,\n",
        "        ground truth answer, and the model's generated answer.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # --- Critical settings for batched generation ---\n",
        "    # Set padding token to EOS token and enable left-padding\n",
        "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
        "    processor.tokenizer.padding_side = \"left\"\n",
        "\n",
        "    # Create an iterator for the batches\n",
        "    num_samples = len(test_dataset)\n",
        "\n",
        "    # Use tqdm for a progress bar if verbose is True\n",
        "    batch_iterator = range(0, num_samples, batch_size)\n",
        "    if verbose:\n",
        "        print(f\"Starting batched inference on {num_samples} samples with batch size {batch_size}...\")\n",
        "        batch_iterator = tqdm(batch_iterator, desc=\"Batch Inference\")\n",
        "\n",
        "    for i in batch_iterator:\n",
        "        # 1. Prepare the current batch\n",
        "        batch_data = test_dataset[i : i + batch_size]\n",
        "        batch_prompts = batch_data['prompt']\n",
        "        batch_ground_truths = batch_data['answer']\n",
        "\n",
        "        # 2. Tokenize the entire batch at once with left-padding\n",
        "        inputs = processor.tokenizer.apply_chat_template(\n",
        "            batch_prompts,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_pt=True,\n",
        "            padding=True,  # Pad sequences to the length of the longest in the batch\n",
        "            max_len=1024,\n",
        "        ).to(device)\n",
        "\n",
        "        # 3. Generate responses for the entire batch in one go\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "\n",
        "        # 4. Decode the generated part of the output\n",
        "        # This is more robust than decoding the whole sequence and stripping the prompt\n",
        "        input_token_length = inputs.shape[1]\n",
        "        generated_tokens = outputs[:, input_token_length:]\n",
        "        model_generated_answers = processor.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # 5. Store the results for the current batch\n",
        "        for j in range(len(batch_prompts)):\n",
        "            results.append({\n",
        "                'prompt': batch_prompts[j],\n",
        "                'ground_truth': batch_ground_truths[j],\n",
        "                'model_answer': model_generated_answers[j].strip() # Use .strip() for clean output\n",
        "            })\n",
        "\n",
        "    # Optional: print a few examples from the final results\n",
        "    if verbose:\n",
        "        print(\"\\n--- Sample of Batched Inference Results ---\")\n",
        "        for res in results[:3]: # Print first 3 results\n",
        "            print(f\"Ground Truth: {res['ground_truth']}\")\n",
        "            print(f\"Model Answer: {res['model_answer']}\\n\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "8QqVAuZolzBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FtBOKjuYlGqs"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate baseline performance\n",
        "\n",
        "This cell calculates the baseline model's accuracy on the test data. This baseline serves as a benchmark to measure the fine-tuned model's performance improvement."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Gemma3Processor, AutoModelForCausalLM\n",
        "\n",
        "# Load model and processor\n",
        "print(\"Loading model and processor...\")\n",
        "ckpt = \"google/medgemma-4b-it\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    ckpt,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = Gemma3Processor.from_pretrained(ckpt)\n",
        "\n",
        "# Run inference on the entire processed dataset\n",
        "inference_results = run_inference_batched(\n",
        "    test_dataset=test_dataset,\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    batch_size=100,\n",
        ")\n",
        "results_df = pd.DataFrame(inference_results)\n",
        "\n",
        "# Defined here again just in case, same as above\n",
        "def extract_xml_answer(text: str) -\u003e str:\n",
        "    answer = text.split(\"\u003canswer\u003e\")[-1]\n",
        "    answer = answer.split(\"\u003c/answer\u003e\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "results_df['model_pred'] = results_df['model_answer'].apply(extract_xml_answer)\n",
        "results_df['correct'] = results_df['ground_truth'] == results_df['model_pred']\n",
        "print('Baseline Accuracy', results_df['correct'].mean())\n",
        "# Save trained_results\n",
        "results_df.to_csv('baseline_results.csv')\n",
        "\n",
        "del model\n",
        "del processor"
      ],
      "metadata": {
        "id": "TRDvIeBUvHOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nLbKev2UlOBY"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate tuned model performance\n",
        "\n",
        "This cell calculates the fine-tuned model's accuracy on the test data. Comparing this with the baseline score shows the improvement from fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import Gemma3Processor\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Define model and processor information (make sure the paths are right!)\n",
        "model_path = \"/content/tuned_medgemma4b/checkpoint-159\"\n",
        "ckpt = \"google/medgemma-4b-it\"\n",
        "\n",
        "# Load Model and Processor\n",
        "print(\"Loading model and processor...\")\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = Gemma3Processor.from_pretrained(ckpt)\n",
        "\n",
        "# Run inference on the entire processed dataset\n",
        "inference_results = run_inference_batched(\n",
        "    test_dataset=test_dataset,\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    batch_size=64,\n",
        ")\n",
        "results_df = pd.DataFrame(inference_results)\n",
        "results_df['model_pred'] = results_df['model_answer'].apply(extract_xml_answer)\n",
        "results_df['correct'] = results_df['ground_truth'] == results_df['model_pred']\n",
        "print('GRPO-tuned Accuracy', results_df['correct'].mean())\n",
        "# Save trained_results\n",
        "results_df.to_csv('trained_results.csv')"
      ],
      "metadata": {
        "id": "AuPqWJoGudfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reproduced MedQA test accuracy after 1 epoch (10 hrs)\n",
        "\n",
        "| Model    | Pre-RL Tuning | Post-RL Tuning |\n",
        "| -------- | ------- | ------- |\n",
        "| gemma-3-4b-it  | 0.479 | 0.535 |\n",
        "| medgemma-4b-it | 0.644 | 0.652 |\n",
        "\n",
        "Observations:\n",
        "\n",
        "- **Both models improved**: Reinforcement learning led to an accuracy increase for both the generalist Gemma and the domain-specific MedGemma models.\n",
        "- **Significant gain for Gemma**: The `gemma-3-4b-it` model shows a substantial improvement of over 5 percentage points, demonstrating the effectiveness of fine-tuning on a general-purpose model.\n",
        "- **Marginal gain for MedGemma**: The `medgemma-4b-it` model, already specialized for medical data, shows a smaller improvement. This suggests it was already highly optimized for this task, leaving less room for gains from this tuning method."
      ],
      "metadata": {
        "id": "i3lzyjX17Nex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional optimizations\n",
        "Note that this notebook is meant to be a starting point. There are numerous optimizations that are not covered under this Colab, including [deepspeed](https://huggingface.co/docs/trl/main/en/deepspeed_integration), [parallelization on multiple nodes](https://huggingface.co/docs/trl/main/en/grpo_trainer#grpo-at-scale-train-a-70b-model-on-multiple-nodes), and more.\n",
        "\n",
        "We recommend checking out [GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer) for further details."
      ],
      "metadata": {
        "id": "jRcRfiEEzkOt"
      }
    },
    {
      "metadata": {
        "id": "IJV_Uw2Vfak8"
      },
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/medgemma/blob/main/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
