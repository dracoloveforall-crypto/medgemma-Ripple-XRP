{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2UTHAMOTkwt"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Prompt MedGemma 1.5 with Whole Slide Digital Pathology Imaging\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/rl_with_trl.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogle-Health%2Fmedgemma%2Fmain%2Fnotebooks%2Frl_with_trl.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/medgemma/blob/main/notebooks/rl_with_trl.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\">\n",
        "      <img alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on Hugging Face\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "This notebook demonstrates how to use digital pathology whole slide imaging to prompt MedGemma 1.5 running on VertexAI.\n",
        "\n",
        "Vertex AI makes it easy to serve your model and make it accessible to the world. Learn more about [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform).\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QHB5n2JR1rV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install [ez-wsi-dicomweb](https://colab.sandbox.google.com/github/GoogleCloudPlatform/EZ-WSI-DICOMweb/blob/main/ez_wsi_demo.ipynb) Python library\n",
        "%%capture\n",
        "! pip install ez-wsi-dicomweb==6.1.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Authenticate Colab User to connect to DICOM store.\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "# There will be a popup asking you to sign in with your user account and approve\n",
        "# access.\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "5YI_XaSKBK2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUVPpaGxsLEC"
      },
      "source": [
        "# Retrieve Digital Pathology Imaging from Imaging Data Commons (IDC)\n",
        "\n",
        "[Imaging Data Commons (IDC)](ttps://datacommons.cancer.gov/repository/imaging-data-commons#) is one of the largest publicly available, de-identified, repositories for cancer imaging. The repository is funded by the [National Cancer Institute (NCI)](https://www.cancer.gov/), an institute of the [National Institutes of Health (NIH)](https://www.nih.gov/), a part of the [U.S. Department of Health and Human Service](https://www.hhs.gov/). IDC contains imaging for all major medical imaging modalities. Imaging is stored within the archive as [DICOM](https://www.dicomstandard.org/). Imaging and its associated metadata can be searched, visualized through the [IDC website](https://portal.imaging.datacommons.cancer.gov/explore/), [BigQuery](https://cloud.google.com/healthcare-api/docs/resources/public-datasets/idc), and can be accessed using DICOMweb ([IDC tutorial](https://learn.canceridc.dev/data/downloading-data/dicomweb-access)).\n",
        "\n",
        "\n",
        "## [DICOM Information Model](https://learn.canceridc.dev/dicom/data-model)\n",
        "[DICOM](https://www.dicomstandard.org/) uniquely identifies imaging using three UIDs, Study Instance UID, Series Instance UID, and a SOP Instance UID. Conceptually, a Study Instance UID can be thought of as the UID that identifies all imaging acquired or generated as a result of a patient exam. Each medical image acquired as part of the exam, (e.g., unique digital pathology image), is identified by a unique Series Instance UID. Each image acquired or generated as part of the an acquisition is, in turn, identified with a unique SOP Instance UID.\n",
        "\n",
        "At its highest magnifications digital pathology images are gigapixel. To enable these images to be rapidly panned and zoomed they are commonly stored as an image pyramid. Each level of the pyramid is stored as a unique image. The colab requests imaging from the the pyramid level that describes imaging at a 10x magnfication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaaAbE6ARS8Q"
      },
      "outputs": [],
      "source": [
        "from ez_wsi_dicomweb import credential_factory\n",
        "from ez_wsi_dicomweb.ml_toolkit import dicom_path\n",
        "from ez_wsi_dicomweb import dicom_web_interface\n",
        "from ez_wsi_dicomweb import dicom_slide\n",
        "from ez_wsi_dicomweb import pixel_spacing\n",
        "\n",
        "# This notebook uses imaging hosted by: Imaging Data Commons (IDC)\n",
        "# This notebook utilizes data generated by the National Cancer Institute\n",
        "# Clinical Proteomic Tumor Analysis Consortium (CPTAC).\n",
        "# Collection: CPTAC-COAD   Case: \t01CO005\n",
        "study_instance_uid = \"2.25.160169972116293851749457993007066200227\"\n",
        "series_instance_uid = \"1.3.6.1.4.1.5962.99.1.178766709.385691431.1640856306549.2.0\"\n",
        "\n",
        "# Read DICOM instance metadata for imaging from IDC\n",
        "#series = f\"https://proxy.imaging.datacommons.cancer.gov/current/viewer-only-no-downloads-see-tinyurl-dot-com-slash-3j3d9jyp/dicomWeb/studies/{study_instance_uid}/series/{series_instance_uid}\"\n",
        "series = f\"https://healthcare.googleapis.com/v1/projects/nci-idc-data/locations/us-central1/datasets/idc/dicomStores/idc-store-v21/dicomWeb/studies/{study_instance_uid}/series/{series_instance_uid}\"\n",
        "dwi = dicom_web_interface.DicomWebInterface(credential_factory.DefaultCredentialFactory())\n",
        "slide = dicom_slide.DicomSlide(dwi, dicom_path.FromString(series))\n",
        "slide.init_slide_frame_cache()\n",
        "\n",
        "# Retrieve imaging from pathology imaging which is ~10x\n",
        "level = slide.get_level_by_pixel_spacing(pixel_spacing.PixelSpacing.FromMagnificationString(\"10X\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "47z6cTSkTTTS"
      },
      "outputs": [],
      "source": [
        "# @title Read patches of digital pathology imaging from the DICOM store.\n",
        "\n",
        "import random\n",
        "\n",
        "from ez_wsi_dicomweb import patch_generator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Maximum number of patches to retrieve from the slide imaging.\n",
        "maximum_number_of_patches = 125\n",
        "patch_size = 896  # MedGemma input image size.\n",
        "\n",
        "# Generate patches from non-overlapping tissue containing regions.\n",
        "patche_generator = patch_generator.DicomPatchGenerator(slide, level, patch_size=patch_size, stride_size=patch_size)\n",
        "sampled_patches = [p for p in patche_generator]\n",
        "sampled_patches = random.sample(sampled_patches, k=min(maximum_number_of_patches, len(sampled_patches)))\n",
        "\n",
        "print(\"Low magnfication view of the slide imaging.\")\n",
        "plt.imshow(patche_generator.get_tissue_mask())\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualization of 3 patches randomly sampled from ~10x imaging\")\n",
        "for p in sampled_patches[:min(3, len(sampled_patches))]:\n",
        "   plt.imshow(p.image_bytes()) # Display the image with a grayscale colormap\n",
        "   plt.axis(\"off\") # Optional: Turn off axis labels and ticks\n",
        "   plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h57n3PWsznxy"
      },
      "outputs": [],
      "source": [
        "# @title Construct MedGemma 1.5 prompt formatted as Chat Completion.\n",
        "\n",
        "\n",
        "# @markdown This section shows how to construct [chat completions](https://platform.openai.com/docs/api-reference/chat) requests to the endpoint using Vertex AI [prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions).\n",
        "\n",
        "import base64\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "def _encode(data: np.ndarray) -> str:\n",
        "  \"\"\"Encode pathology patch imaging inline in prompt.\"\"\"\n",
        "  # Image format pathology patches are encoded as.\n",
        "  # options: \"jpeg\" or \"png\"\n",
        "  image_format = \"jpeg\"\n",
        "  with PIL.Image.fromarray(data) as img:\n",
        "      with io.BytesIO() as img_bytes:\n",
        "        img.save(img_bytes, format=image_format)\n",
        "        encoded_string = base64.b64encode(img_bytes.getvalue()).decode(\"utf-8\")\n",
        "  return f\"data:image/{image_format};base64,{encoded_string}\"\n",
        "\n",
        "# @markdown **Prompt:** Provide a brief descriptive text for the set of pathology patches extracted from a pathology slide. Consider the tissue type and procedure (below) when deciding what to include in the descriptive text.\\ncolon, resection:\n",
        "\n",
        "prompt = \"Provide a brief descriptive text for the set of pathology patches extracted from a pathology slide. Consider the tissue type and procedure (below) when deciding what to include in the descriptive text.\\ncolon, resection:\"\n",
        "\n",
        "# Generate chat completion formatted prompt.\n",
        "content = [{\"type\": \"text\", \"text\": prompt}]\n",
        "for p in sampled_patches:\n",
        "  content.append({\"type\": \"image_url\", \"image_url\": {\"url\": _encode(p.image_bytes())}})\n",
        "\n",
        "instance = {\n",
        "        \"@requestFormat\": \"chatCompletions\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
        "        \"max_tokens\": 500,\n",
        "        \"temperature\": 0\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyOnyVYYavZP"
      },
      "outputs": [],
      "source": [
        "# @title Display full MedGemma 1.5 prompt.\n",
        "import json\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "def truncate_prompt(obj, max_len):\n",
        "  # Clip strings in prompt to avoid displaying excessively large content in colab notebook.\n",
        "  if isinstance(obj, dict):\n",
        "    return {k: truncate_prompt(v, max_len) for k, v in obj.items()}\n",
        "  elif isinstance(obj, list):\n",
        "    return [truncate_prompt(elem, max_len) for elem in obj]\n",
        "  elif isinstance(obj, str) and len(obj) > max_len:\n",
        "    return obj[:max_len] + \"...\"  # Add ellipsis for truncated strings\n",
        "  return obj\n",
        "\n",
        "txt = json.dumps(truncate_prompt(instance, 100), indent=4, sort_keys=True)\n",
        "display(Markdown(f\"```json\\n{txt}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPCLQAwZjDjS"
      },
      "outputs": [],
      "source": [
        "# @title Configure CoLab to call MedGemma 1.5 running in Vertex AI\n",
        "\n",
        "# @markdown #### Prerequisites\n",
        "\n",
        "# @markdown 1. Make sure that [billing is enabled](https://cloud.google.com/billing/docs/how-to/modify-project) for your project.\n",
        "\n",
        "# @markdown 2. Make sure that either the Compute Engine API is enabled or that you have the [Service Usage Admin](https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin) (`roles/serviceusage.serviceUsageAdmin`) role to enable the API.\n",
        "\n",
        "# @markdown This section sets the default Google Cloud project and enables the Compute Engine API (if not already enabled), and initializes the Vertex AI API.\n",
        "\n",
        "\n",
        "# @title Import packages and define common functions\n",
        "import os\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "Google_Cloud_Project = \"\"  # @param {type: \"string\", placeholder:\"e.g., my_project_name\"}\n",
        "\n",
        "# @markdown To get [online predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions), you will need a MedGemma [Vertex AI Endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment) that has been deployed from Model Garden. If you have not already done so, go to the [MedGemma model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medgemma) and click \"Deploy options > Vertex AI\" to deploy the model.\n",
        "\n",
        "# @markdown **Note:** The examples in this notebook are intended to be used with instruction-tuned variants. Make sure to use an instruction-tuned model variant to run this notebook.\n",
        "\n",
        "# @markdown This section gets the Vertex AI Endpoint resource that you deployed from Model Garden to use for online predictions.\n",
        "\n",
        "# @markdown Fill in the endpoint ID and region below. You can find your deployed endpoint on the [Vertex AI online prediction page](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "\n",
        "\n",
        "ENDPOINT_ID = \"\"  # @param {type: \"string\", placeholder:\"e.g. 123456789\"}\n",
        "ENDPOINT_REGION = \"\"  # @param {type: \"string\", placeholder:\"e.g. us-central1\"}\n",
        "\n",
        "# @markdown **Note:** The colab requires dedicated [Vertex AI endpoint](https://cloud.google.com/blog/products/ai-machine-learning/reliable-ai-with-vertex-ai-prediction-dedicated-endpoints?e=48754805).\n",
        "\n",
        "os.environ[\"CLOUDSDK_CORE_PROJECT\"] = Google_Cloud_Project\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = Google_Cloud_Project\n",
        "os.environ[\"GOOGLE_CLOUD_REGION\"] = ENDPOINT_REGION\n",
        "\n",
        "# Enable the Compute Engine API, if not already.\n",
        "print(\"Enabling Compute Engine API.\")\n",
        "! gcloud services enable compute.googleapis.com\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=os.environ[\"GOOGLE_CLOUD_PROJECT\"],\n",
        "                location=os.environ[\"GOOGLE_CLOUD_REGION\"])\n",
        "\n",
        "endpoint = aiplatform.Endpoint(\n",
        "    endpoint_name=ENDPOINT_ID,\n",
        "    project=Google_Cloud_Project,\n",
        "    location=ENDPOINT_REGION,\n",
        ")\n",
        "\n",
        "# Use the endpoint name to check that you are using an appropriate model variant.\n",
        "# These checks are based on the default endpoint name from the Model Garden\n",
        "# deployment settings.\n",
        "ENDPOINT_NAME = endpoint.display_name\n",
        "if \"pt\" in ENDPOINT_NAME:\n",
        "    raise ValueError(\n",
        "        \"The examples in this notebook are intended to be used with \"\n",
        "        \"instruction-tuned variants. Please use an instruction-tuned model.\"\n",
        "    )\n",
        "if \"text\" in ENDPOINT_NAME:\n",
        "    raise ValueError(\n",
        "        \"You are using a text-only variant which does not support multimodal\"\n",
        "        \" inputs. Please proceed to the 'Run inference on text only' section.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DAf9TNK1gb5"
      },
      "outputs": [],
      "source": [
        "# @title # Call MedGemma 1.5 and return prediction\n",
        "\n",
        "response = endpoint.raw_predict(\n",
        "    body=json.dumps(instance).encode(\"utf-8\"), use_dedicated_endpoint=True,\n",
        "    headers={\"Content-Type\": \"application/json\"}\n",
        ")\n",
        "response.raise_for_status()\n",
        "medgemma_response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{medgemma_response}\\n\\n---\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hitW2-3UFoBV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
