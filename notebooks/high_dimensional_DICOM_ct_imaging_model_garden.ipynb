{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "last_runtime": {
        "build_target": "//medical/radiology/colab:notebook3",
        "kind": "private"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2UTHAMOTkwt"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Prompt MedGemma 1.5 with DICOM Computed Tomography (CT) Imaging\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/rl_with_trl.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogle-Health%2Fmedgemma%2Fmain%2Fnotebooks%2Frl_with_trl.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/medgemma/blob/main/notebooks/rl_with_trl.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\">\n",
        "      <img alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on Hugging Face\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "This notebook demonstrates how 3D representation of [computed tomography (CT)](https://www.nibib.nih.gov/science-education/science-topics/computed-tomography-ct) imaging can be used to prompt MedGemma 1.5 running on Vertex AI. This notebook is for educational purposes only to show the baseline functionality of MedGemma 1.5. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment for any disease or condition, and should not be used for medical advice. See [HAI-DEF Terms of Use](https://developers.google.com/health-ai-developer-foundations/terms) for more information.\n",
        "\n",
        "Vertex AI makes it easy to serve your model and make it accessible to the world. Learn more about [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform).\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing); use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install pydicom Python library\n",
        "%%capture\n",
        "! pip install pydicom"
      ],
      "metadata": {
        "id": "2QHB5n2JR1rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Authenticate Colab User to connect to DICOM store.\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "# There will be a popup asking you to sign in with your user account and approve\n",
        "# access.\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "7BTl8wSSML9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve Imaging Data Commons (IDC) CT Imaging\n",
        "\n",
        "[Imaging Data Commons (IDC)](ttps://datacommons.cancer.gov/repository/imaging-data-commons#) is one of the largest publicly available, de-identified, repositories for cancer imaging. The repository is funded by the [National Cancer Institute (NCI)](https://www.cancer.gov/), an institute of the [National Institutes of Health (NIH)](https://www.nih.gov/), a part of the [U.S. Department of Health and Human Service](https://www.hhs.gov/). IDC contains imaging for all major medical imaging modalities. Imaging is stored within the archive as [DICOM](https://www.dicomstandard.org/). Imaging and its associated metadata can be searched, visualized through the [IDC website](https://portal.imaging.datacommons.cancer.gov/explore/), [BigQuery](https://cloud.google.com/healthcare-api/docs/resources/public-datasets/idc), and can be accessed using DICOMweb ([IDC tutorial](https://learn.canceridc.dev/data/downloading-data/dicomweb-access)). For the purposes of this colab we have mirrored the required imaging in a Google DICOM store.\n",
        "\n",
        "DICOM is the medical imaging format generated by CT scanners.\n",
        "[DICOM](https://www.dicomstandard.org/) images are uniquely identified by three UIDs, Study Instance UID, Series Instance UID, and a SOP Instance UID.  The Colab retrieve a CT imaging from IDC by downloading all of the images associated with a CT scan. Conceptually, a Study Instance UID can be thought of as the UID that identifies all imaging acquired or generated as a result of a patient exam. Each medical image acquired as part of the exam, (e.g., CT acquisition), is identified by a unique Series Instance UID. Each image acquired or generated as part of the an acquisition is, in turn, identified with a unique SOP Instance UID.\n",
        "\n",
        "CT imaging is commonly represented as an ordered collection of 2D images (slices). Each slice is represented by a 2D image that describes an axial cross section (volume) of the imaged body.\n",
        "\n"
      ],
      "metadata": {
        "id": "KUVPpaGxsLEC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaaAbE6ARS8Q"
      },
      "outputs": [],
      "source": [
        "# @title Read metadata for the DICOM instances in a series.\n",
        "from typing import Sequence\n",
        "\n",
        "import google.auth\n",
        "import google.auth.transport\n",
        "import pydicom\n",
        "import requests\n",
        "\n",
        "\n",
        "def get_series_dicomweb_url() -> str:\n",
        "  return f\"https://healthcare.googleapis.com/v1/projects/hai-cd3-foundations/locations/us-central1/datasets/public/dicomStores/test-images/dicomWeb/studies/{study_instance_uid}/series/{series_instance_uid}\"\n",
        "\n",
        "def get_sop_instance_dicomweb_urls(study_instance_uid: str, series_instance_uid: str) -> Sequence[str]:\n",
        "  # Read DICOM instance metadata for imaging\n",
        "  # Get oauth credentials to\n",
        "  auth_credentials = google.auth.default(['https://www.googleapis.com/auth/cloud-healthcare'])[0]\n",
        "  auth_credentials.refresh(google.auth.transport.requests.Request())\n",
        "  headers = {\"Authorization\" : f\"Bearer {auth_credentials.token}\"}\n",
        "  series = get_series_dicomweb_url(study_instance_uid, series_instance_uid)\n",
        "  metadata = requests.get(f\"{series}/instances\", headers=headers).json()\n",
        "  dicom_instances = [pydicom.Dataset.from_json(i) for i in metadata]\n",
        "  # Order instance metadata by instance number\n",
        "  dicom_instances = sorted(dicom_instances, key=lambda i: int(i.InstanceNumber))\n",
        "\n",
        "  # Sample 85 slices equallly across CT volume\n",
        "  MAX_SLICE = 85\n",
        "  if len(dicom_instances) > MAX_SLICE:\n",
        "    dicom_instances = [dicom_instances[int(round(i /(MAX_SLICE - 1) * (len(dicom_instances)-1)))] for i in range(MAX_SLICE)]\n",
        "  return [f'{series}/instances/{dcm.SOPInstanceUID}' for dcm in dicom_instances]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Construct MedGemma 1.5 prompt formatted as Chat Completion.\n",
        "\n",
        "# This notebook uses imaging hosted in the Imaging Data Commons (IDC) archive\n",
        "# This notebook utilizes data from The Cancer Imaging Archive (TCIA).\n",
        "# https://www.cancerimagingarchive.net/collection/colorectal-liver-metastases/\n",
        "# Liver CRLM - CT 1026\n",
        "study_instance_uid = \"1.3.6.1.4.1.14519.5.2.1.9203.8273.982856921320609617394372605436\"\n",
        "series_instance_uid = \"1.3.6.1.4.1.14519.5.2.1.9203.8273.275179554444442893192427753220\"\n",
        "\n",
        "# @markdown This section shows how to construct chat completions style requests to the endpoint using Vertex AI [prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions).\n",
        "\n",
        "prompt_using = 'SOPInstanceUID' # @param ['SOPInstanceUID', 'SeriesInstanceUID']\n",
        "\n",
        "# Generate chat completion formatted prompt.\n",
        "\n",
        "if prompt_using == 'SOPInstanceUID':\n",
        "  dicom_source = get_sop_instance_dicomweb_urls(study_instance_uid, series_instance_uid)\n",
        "else:\n",
        "  dicom_source = get_series_dicomweb_url(study_instance_uid, series_instance_uid)\n",
        "\n",
        "instruction = (\"You are an instructor teaching medical students. You are \"\n",
        "               \"analyzing a contiguous block of CT slices from the center of \"\n",
        "               \"the abdomen. Please review the slices provided below \"\n",
        "               \"carefully.\")\n",
        "content = [{\"type\": \"text\", \"text\": instruction}]\n",
        "content.append({\"type\": \"image_dicom\", \"image_dicom\": {\"dicom_source\": dicom_source}})\n",
        "query_text = (\"\\n\\nBased on the visual evidence in the slices provided above, \"\n",
        "              \"is this image a good teaching example of liver pathology? \"\n",
        "              \"Comment on hypodense lesions or other hepatic irregularities. \"\n",
        "              \"Do not comment on findings outside the liver. Please provide \"\n",
        "              \"your reasoning and conclude with a 'Final Answer: yes' or \"\n",
        "              \"'Final Answer: no'.\")\n",
        "content.append({\"type\": \"text\", \"text\": query_text})\n",
        "\n",
        "instance = {\n",
        "    \"@requestFormat\": \"chatCompletions\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
        "    \"max_tokens\": 500,\n",
        "    \"temperature\": 0\n",
        "}"
      ],
      "metadata": {
        "id": "h57n3PWsznxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Display MedGemma 1.5 prompt.\n",
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "txt = json.dumps(instance, indent=4, sort_keys=True)\n",
        "display(Markdown(f'```json\\n{txt}'))"
      ],
      "metadata": {
        "id": "bRyQWvSn5Cd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configure CoLab to call MedGemma 1.5 running in Vertex AI\n",
        "\n",
        "# @markdown #### Prerequisites\n",
        "\n",
        "# @markdown 1. Make sure that [billing is enabled](https://cloud.google.com/billing/docs/how-to/modify-project) for your project.\n",
        "\n",
        "# @markdown 2. Make sure that either the Compute Engine API is enabled or that you have the [Service Usage Admin](https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin) (`roles/serviceusage.serviceUsageAdmin`) role to enable the API.\n",
        "\n",
        "# @markdown This section sets the default Google Cloud project and enables the Compute Engine API (if not already enabled), and initializes the Vertex AI API.\n",
        "\n",
        "import os\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "Google_Cloud_Project = \"\"  # @param {type: \"string\", placeholder:\"e.g. MyProject\"}\n",
        "\n",
        "# @markdown To get [online predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions), you will need a MedGemma [Vertex AI Endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment) that has been deployed from Model Garden. If you have not already done so, go to the [MedGemma model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medgemma) and click \"Deploy options > Vertex AI\" to deploy the model.\n",
        "\n",
        "# @markdown **Note:** The examples in this notebook are intended to be used with instruction-tuned variants. Make sure to use an instruction-tuned model variant to run this notebook.\n",
        "\n",
        "# @markdown This section gets the Vertex AI Endpoint resource that you deployed from Model Garden to use for online predictions.\n",
        "\n",
        "# @markdown Fill in the endpoint ID and region below. You can find your deployed endpoint on the [Vertex AI online prediction page](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "\n",
        "\n",
        "ENDPOINT_ID = \"\"  # @param {type: \"string\", placeholder:\"e.g. 123456789\"}\n",
        "ENDPOINT_REGION = \"\"  # @param {type: \"string\", placeholder:\"e.g. us-central1\"}\n",
        "\n",
        "# @markdown **Note:** The colab requires dedicated [Vertex AI endpoint](https://cloud.google.com/blog/products/ai-machine-learning/reliable-ai-with-vertex-ai-prediction-dedicated-endpoints?e=48754805).\n",
        "\n",
        "os.environ[\"CLOUDSDK_CORE_PROJECT\"] = Google_Cloud_Project\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = Google_Cloud_Project\n",
        "os.environ[\"GOOGLE_CLOUD_REGION\"] = ENDPOINT_REGION\n",
        "\n",
        "# Enable the Compute Engine API, if not already.\n",
        "print(\"Enabling Compute Engine API.\")\n",
        "! gcloud services enable compute.googleapis.com\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=os.environ[\"GOOGLE_CLOUD_PROJECT\"],\n",
        "                location=os.environ[\"GOOGLE_CLOUD_REGION\"])\n",
        "\n",
        "endpoint = aiplatform.Endpoint(\n",
        "    endpoint_name=ENDPOINT_ID,\n",
        "    project=Google_Cloud_Project,\n",
        "    location=ENDPOINT_REGION,\n",
        ")\n",
        "\n",
        "# Use the endpoint name to check that you are using an appropriate model variant.\n",
        "# These checks are based on the default endpoint name from the Model Garden\n",
        "# deployment settings.\n",
        "ENDPOINT_NAME = endpoint.display_name\n",
        "if \"pt\" in ENDPOINT_NAME:\n",
        "    raise ValueError(\n",
        "        \"The examples in this notebook are intended to be used with \"\n",
        "        \"instruction-tuned variants. Please use an instruction-tuned model.\"\n",
        "    )\n",
        "if \"text\" in ENDPOINT_NAME:\n",
        "    raise ValueError(\n",
        "        \"You are using a text-only variant which does not support multimodal\"\n",
        "        \" inputs. Please proceed to the 'Run inference on text only' section.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "MPCLQAwZjDjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Call MedGemma 1.5 and return prediction\n",
        "\n",
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "response = endpoint.raw_predict(\n",
        "    body=json.dumps(instance).encode('utf-8'), use_dedicated_endpoint=True,\n",
        "    headers={'Content-Type': 'application/json'}\n",
        ")\n",
        "response.raise_for_status()\n",
        "medgemma_response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{medgemma_response}\\n\\n---\"))"
      ],
      "metadata": {
        "id": "4DAf9TNK1gb5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
