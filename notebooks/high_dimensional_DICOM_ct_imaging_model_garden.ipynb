{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "last_runtime": {
        "build_target": "//medical/radiology/colab:notebook3",
        "kind": "private"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2UTHAMOTkwt"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Prompt MedGemma 1.5 with DICOM Computed Tomography (CT) Imaging\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/rl_with_trl.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogle-Health%2Fmedgemma%2Fmain%2Fnotebooks%2Frl_with_trl.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/medgemma/blob/main/notebooks/rl_with_trl.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\">\n",
        "      <img alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on Hugging Face\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "This notebook demonstrates how to use 3D representation of [computed tomography (CT)](https://www.nibib.nih.gov/science-education/science-topics/computed-tomography-ct) imaging to prompt MedGemma 1.5 running on VertexAI.\n",
        "\n",
        "Vertex AI makes it easy to serve your model and make it accessible to the world. Learn more about [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform).\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing); use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install pydicom Python library\n",
        "%%capture\n",
        "! pip install pydicom"
      ],
      "metadata": {
        "id": "2QHB5n2JR1rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Authenticate Colab User to connect to DICOM store.\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "# There will be a popup asking you to sign in with your user account and approve\n",
        "# access.\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "7BTl8wSSML9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve CT Imaging from Imaging Data Commons (IDC)\n",
        "\n",
        "[Imaging Data Commons (IDC)](ttps://datacommons.cancer.gov/repository/imaging-data-commons#) is one of the largest publicly available, de-identified, repositories for cancer imaging. The repository is funded by the [National Cancer Institute (NCI)](https://www.cancer.gov/), an institute of the [National Institutes of Health (NIH)](https://www.nih.gov/), a part of the [U.S. Department of Health and Human Service](https://www.hhs.gov/). IDC contains imaging for all major medical imaging modalities. Imaging is stored within the archive as [DICOM](https://www.dicomstandard.org/). Imaging and its associated metadata can be searched, visualized through the [IDC website](https://portal.imaging.datacommons.cancer.gov/explore/), [BigQuery](https://cloud.google.com/healthcare-api/docs/resources/public-datasets/idc), and can be accessed using DICOMweb ([IDC tutorial](https://learn.canceridc.dev/data/downloading-data/dicomweb-access)).\n",
        "\n",
        "DICOM is the medical imaging format generated by CT scanners.\n",
        "[DICOM](https://www.dicomstandard.org/) images are uniquely identified by three UIDs, Study Instance UID, Series Instance UID, and a SOP Instance UID.  The CoLab retrieve a CT imaging from IDC by downloading all of the images associated with a CT scan. Conceptually, a Study Instance UID can be thought of as the UID that identifies all imaging acquired or generated as a result of a patient exam. Each medical image acquired as part of the exam, (e.g., CT acquisition), is identified by a unique Series Instance UID. Each image acquired or generated as part of the an acquisition is, in turn, identified with a unique SOP Instance UID.\n",
        "\n",
        "CT imaging is commonly represented as an ordered collection of 2D images (slices). Each slice is represented by a 2D image that describes an axial cross section (volume) of the imaged body.\n",
        "\n"
      ],
      "metadata": {
        "id": "KUVPpaGxsLEC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaaAbE6ARS8Q"
      },
      "outputs": [],
      "source": [
        "# @title Read metadata for the DICOM instances in a series.\n",
        "\n",
        "import pydicom\n",
        "import requests\n",
        "\n",
        "# This notebook uses imaging hosted by: Imaging Data Commons (IDC)\n",
        "# This notebook utilizes data from The Cancer Imaging Archive (TCIA).\n",
        "# Collection: \tC4KC-KiTS   Case: KiTS-00004\n",
        "study_instance_uid = '1.3.6.1.4.1.14519.5.2.1.6919.4624.108154519927657031748507898966'\n",
        "series_instance_uid = '1.3.6.1.4.1.14519.5.2.1.6919.4624.203718711189211521545414320103'\n",
        "\n",
        "# Read DICOM instance metadata for imaging from IDC\n",
        "series = f'https://proxy.imaging.datacommons.cancer.gov/current/viewer-only-no-downloads-see-tinyurl-dot-com-slash-3j3d9jyp/dicomWeb/studies/{study_instance_uid}/series/{series_instance_uid}'\n",
        "metadata = requests.get(f'{series}/instances').json()\n",
        "dicom_instances = [pydicom.Dataset.from_json(i) for i in metadata]\n",
        "# Order instance metadata by instance number\n",
        "dicom_instances = sorted(dicom_instances, key=lambda i: int(i.InstanceNumber))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load slice imaging for all DICOM instances in the series.\n",
        "\n",
        "# @markdown Background: Each slice of a CT imaging is typically represented in DICOM as its own instance.\n",
        "import io\n",
        "\n",
        "ct_volume_slices = []\n",
        "headers = {'Accept': 'application/dicom; transfer-syntax=*'}\n",
        "for i in dicom_instances:\n",
        "  # Download DICOM instance\n",
        "  pixel_data = requests.get(f'{series}/instances/{i.SOPInstanceUID}', headers=headers).content\n",
        "  with io.BytesIO(pixel_data) as pd:\n",
        "    # Read DICOM instance from in memory buffer\n",
        "    with pydicom.dcmread(pd) as dcm:\n",
        "      # Extract CT Slice pixel data from and scale to hounsfield units.(\n",
        "      ct_volume_slices.append(pydicom.pixels.apply_rescale(dcm.pixel_array, dcm))\n"
      ],
      "metadata": {
        "id": "47z6cTSkTTTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define windowing for CT imaging\n",
        "\n",
        "The voxels encoded within CT imaging are typically expressed as signed 16-bit [hounsfield units (HU)](https://en.wikipedia.org/wiki/Hounsfield_scale), one value per-voxel. CT images are commonly visualized as a grayscale image. The imaging is typically [windowed](https://radiopaedia.org/articles/windowing-ct) for human reading tasks to increase the contrast across a task specific diagnostic range.\n",
        "\n",
        "The MedGemma image encoder accepts RGB, 8-bits per channel, imaging as input. MedGemma has also been trained to interpret CT imaging where the RGB channels of input imaging correspond to a novel windowing representation to enable it to interpret multiple representations of CT imaging simultaneously.  Specifically, MedGemma 1.5 has been trained with the components defined as follows:\n",
        "  * Red (component 0):  Wide window; range: -1024 HU ([air](https://en.wikipedia.org/wiki/Hounsfield_scale)) to 1024 HU ([above bone](https://en.wikipedia.org/wiki/Hounsfield_scale))\n",
        "  * Green (component 1):  Soft tissue window; range: 135 HU ([fat](https://en.wikipedia.org/wiki/Hounsfield_scale)) to 215 HU ([start of bone](https://en.wikipedia.org/wiki/Hounsfield_scale))\n",
        "  * Blue (component 2): Brain window; range: 0 HU ([water](https://en.wikipedia.org/wiki/Hounsfield_scale)) to 80 HU  ([brain](https://en.wikipedia.org/wiki/Hounsfield_scale))\n",
        "\n",
        "  Because, each of the RGB channels in the prompt imaging correspond to a windowing prompt imaging prepared using this method will visually appear color.\n",
        "\n"
      ],
      "metadata": {
        "id": "sQNpMV2FFLIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import IPython.display\n",
        "import PIL.Image\n",
        "\n",
        "def norm(ct_vol: np.ndarray, min: float, max: float) -> np.ndarray:\n",
        "  \"\"\"Window and normalize CT imaging Houndsfield values to values 0 - 255.\"\"\"\n",
        "  ct_vol = np.clip(ct_vol, min, max)  # Clip the imaging value range\n",
        "  ct_vol = ct_vol.astype(np.float32)\n",
        "  ct_vol -= min\n",
        "  ct_vol /= (max - min) # Norm to values between 0 - 1.0\n",
        "  ct_vol *= 255.0  # Norm to values been 0 - 255.0\n",
        "  return ct_vol\n",
        "\n",
        "def window(ct_vol: np.ndarray, dcm: pydicom.Dataset) -> np.ndarray:\n",
        "  # Window CT slice imaging with three windows (wide, mediastinum(chest), brain)\n",
        "  # Imaging will appear color when visualized, RGB channels contain different\n",
        "  # representations of the data.\n",
        "  window_clips = [(-1024, 1024), (-135, 215), (0, 80)]\n",
        "  return np.stack([norm(ct_vol, clip[0], clip[1]) for clip in window_clips], axis=-1)\n",
        "\n",
        "# Window CT Slice Data.\n",
        "normalized_ct_volume_slices = []\n",
        "for ct_slice in ct_volume_slices:\n",
        "  windowed_slice = window(ct_slice, dcm)\n",
        "  # Round slice voxels to nearest integer number.\n",
        "  windowed_slice = np.round(windowed_slice, 0).astype(np.uint8)\n",
        "  normalized_ct_volume_slices.append(windowed_slice)\n",
        "\n",
        "# @markdown **Visualize CT Slice Windowing**\n",
        "\n",
        "# Visualize windowed CT Slices.\n",
        "ct_slice_images = [PIL.Image.fromarray(ct_slice) for ct_slice in normalized_ct_volume_slices]\n",
        "\n",
        "# Save slice images as animated GIF\n",
        "with io.BytesIO() as gif_bytes:\n",
        "  ct_slice_images[0].save(gif_bytes, format='GIF', loop=0, save_all=True, append_images=ct_slice_images[1:], optimize=False, duration=len(normalized_ct_volume_slices)*3)\n",
        "  ct_slice_animation = gif_bytes.getvalue()\n",
        "# Display animated gif in colab\n",
        "IPython.display.display(IPython.display.Image(data=ct_slice_animation, format='GIF'))\n"
      ],
      "metadata": {
        "id": "iTu1nQneLEvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Construct MedGemma 1.5 prompt formatted as Chat Completion.\n",
        "\n",
        "\n",
        "# @markdown This section shows how to construct [chat completions](https://platform.openai.com/docs/api-reference/chat) requests to the endpoint using Vertex AI [prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions).\n",
        "\n",
        "import base64\n",
        "import PIL.Image\n",
        "\n",
        "def _encode(data: np.ndarray) -> str:\n",
        "  \"\"\"Encode CT slice imaging inline in prompt.\"\"\"\n",
        "  # Image format to encode ct slice images as.\n",
        "  # options: 'jpeg' or 'png'\n",
        "  format = 'jpeg'\n",
        "  with io.BytesIO() as img_bytes:\n",
        "    with PIL.Image.fromarray(data) as img:\n",
        "      img.save(img_bytes, format=format)\n",
        "    img_bytes.seek(0)\n",
        "    encoded_string = base64.b64encode(img_bytes.getbuffer()).decode(\"utf-8\")\n",
        "  return f\"data:image/{format};base64,{encoded_string}\"\n",
        "\n",
        "\n",
        "prompt = 'Is there evidence of renal carcinoma in this CT scan volume?' # @param ['Is there evidence of renal carcinoma in this CT scan volume?', 'Is there evidence of arterial calcification in this ct volume']\n",
        "\n",
        "# Generate chat completion formatted prompt.\n",
        "content = []\n",
        "for slice_number, ct_slice in enumerate(normalized_ct_volume_slices, 1):\n",
        "  content.append({\"type\": \"image_url\", \"image_url\": {\"url\": _encode(ct_slice)}})\n",
        "  content.append({\"type\": \"text\", \"text\": f'SLICE {slice_number}'})\n",
        "content.append({\"type\": \"text\", \"text\": prompt})\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": content\n",
        "    }\n",
        "]\n",
        "\n",
        "instance = {\n",
        "    \"@requestFormat\": \"chatCompletions\",\n",
        "    \"messages\": messages,\n",
        "    \"max_tokens\": 500,\n",
        "    \"temperature\": 0\n",
        "}"
      ],
      "metadata": {
        "id": "h57n3PWsznxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Display MedGemma 1.5 prompt.\n",
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "def truncate_prompt(obj, max_len):\n",
        "  # Clip strings in prompt to avoid displaying excessively large content in colab notebook.\n",
        "  if isinstance(obj, dict):\n",
        "    return {k: truncate_prompt(v, max_len) for k, v in obj.items()}\n",
        "  elif isinstance(obj, list):\n",
        "    return [truncate_prompt(elem, max_len) for elem in obj]\n",
        "  elif isinstance(obj, str) and len(obj) > max_len:\n",
        "    return obj[:max_len] + \"...\"  # Add ellipsis for truncated strings\n",
        "  return obj\n",
        "\n",
        "\n",
        "txt = json.dumps(truncate_prompt(instance, 100), indent=4, sort_keys=True)\n",
        "display(Markdown(f'```json\\n{txt}'))"
      ],
      "metadata": {
        "id": "bRyQWvSn5Cd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configure CoLab to call MedGemma 1.5 running in Vertex AI\n",
        "\n",
        "# @markdown #### Prerequisites\n",
        "\n",
        "# @markdown 1. Make sure that [billing is enabled](https://cloud.google.com/billing/docs/how-to/modify-project) for your project.\n",
        "\n",
        "# @markdown 2. Make sure that either the Compute Engine API is enabled or that you have the [Service Usage Admin](https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin) (`roles/serviceusage.serviceUsageAdmin`) role to enable the API.\n",
        "\n",
        "# @markdown This section sets the default Google Cloud project and enables the Compute Engine API (if not already enabled), and initializes the Vertex AI API.\n",
        "\n",
        "import os\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "Google_Cloud_Project = \"\"  # @param {type: \"string\", placeholder:\"e.g. MyProject\"}\n",
        "\n",
        "# @markdown To get [online predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions), you will need a MedGemma [Vertex AI Endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment) that has been deployed from Model Garden. If you have not already done so, go to the [MedGemma model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medgemma) and click \"Deploy options > Vertex AI\" to deploy the model.\n",
        "\n",
        "# @markdown **Note:** The examples in this notebook are intended to be used with instruction-tuned variants. Make sure to use an instruction-tuned model variant to run this notebook.\n",
        "\n",
        "# @markdown This section gets the Vertex AI Endpoint resource that you deployed from Model Garden to use for online predictions.\n",
        "\n",
        "# @markdown Fill in the endpoint ID and region below. You can find your deployed endpoint on the [Vertex AI online prediction page](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "\n",
        "\n",
        "ENDPOINT_ID = \"\"  # @param {type: \"string\", placeholder:\"e.g. 123456789\"}\n",
        "ENDPOINT_REGION = \"\"  # @param {type: \"string\", placeholder:\"e.g. us-central1\"}\n",
        "\n",
        "# @markdown **Note:** The colab requires dedicated [Vertex AI endpoint](https://cloud.google.com/blog/products/ai-machine-learning/reliable-ai-with-vertex-ai-prediction-dedicated-endpoints?e=48754805).\n",
        "\n",
        "os.environ[\"CLOUDSDK_CORE_PROJECT\"] = Google_Cloud_Project\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = Google_Cloud_Project\n",
        "os.environ[\"GOOGLE_CLOUD_REGION\"] = ENDPOINT_REGION\n",
        "\n",
        "# Enable the Compute Engine API, if not already.\n",
        "print(\"Enabling Compute Engine API.\")\n",
        "! gcloud services enable compute.googleapis.com\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=os.environ[\"GOOGLE_CLOUD_PROJECT\"],\n",
        "                location=os.environ[\"GOOGLE_CLOUD_REGION\"])\n",
        "\n",
        "endpoint = aiplatform.Endpoint(\n",
        "    endpoint_name=ENDPOINT_ID,\n",
        "    project=Google_Cloud_Project,\n",
        "    location=ENDPOINT_REGION,\n",
        ")\n",
        "\n",
        "# Use the endpoint name to check that you are using an appropriate model variant.\n",
        "# These checks are based on the default endpoint name from the Model Garden\n",
        "# deployment settings.\n",
        "ENDPOINT_NAME = endpoint.display_name\n",
        "if \"pt\" in ENDPOINT_NAME:\n",
        "    raise ValueError(\n",
        "        \"The examples in this notebook are intended to be used with \"\n",
        "        \"instruction-tuned variants. Please use an instruction-tuned model.\"\n",
        "    )\n",
        "if \"text\" in ENDPOINT_NAME:\n",
        "    raise ValueError(\n",
        "        \"You are using a text-only variant which does not support multimodal\"\n",
        "        \" inputs. Please proceed to the 'Run inference on text only' section.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "MPCLQAwZjDjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Call MedGemma 1.5 and return prediction\n",
        "\n",
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "response = endpoint.raw_predict(\n",
        "    body=json.dumps(instance).encode('utf-8'), use_dedicated_endpoint=True,\n",
        "    headers={'Content-Type': 'application/json'}\n",
        ")\n",
        "response.raise_for_status()\n",
        "medgemma_response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{medgemma_response}\\n\\n---\"))"
      ],
      "metadata": {
        "id": "4DAf9TNK1gb5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
